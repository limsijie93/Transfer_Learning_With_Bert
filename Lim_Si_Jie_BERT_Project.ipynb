{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4-3Ju5c2PI8d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning Using BERT \n",
        "\n",
        "## Project Summary: \n",
        "This project aims to use the Google released Bert model for transfer learning. The idea is similar to transfer learning using image recognition model (e.g. VGG, ResNet) by adding a classifer head to the underlying outputs from the base model.\n",
        "\n",
        "### Problem Statement:\n",
        "How can we use transfer learning for an NLP problem to improve classification results? \n",
        "\n",
        "In particular, I am going to tackle the 'Quora Insincere Question Classification' problem on Kaggle.\n",
        "\n",
        "Link: https://www.kaggle.com/c/quora-insincere-questions-classification\n",
        "\n",
        "#### Dataset used:\n",
        "Kaggle competition dataset - Quora Insincere Question Classification\n",
        "\n",
        "#### Resources used:\n",
        "Colab\n",
        "\n",
        "#### Code implemented in:\n",
        "PyTorch\n",
        "\n",
        "#### Credit: Lim Si Jie"
      ]
    },
    {
      "metadata": {
        "id": "RhBc17CuULmV",
        "colab_type": "code",
        "outputId": "82c9edc0-fcc1-47ff-b395-be1c3fbb88f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert\n",
        "!pip install \n",
        "!pip install kaggle --u"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.105)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.105 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.105)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.105->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.105->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.105->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "ambiguous option: --u (--upgrade, --upgrade-strategy, --use-pep517, --user?)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OuOiIrMPZFF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive to Google's Linux VM (Colab)"
      ]
    },
    {
      "metadata": {
        "id": "YlgNAF-_ZJBK",
        "colab_type": "code",
        "outputId": "0f030b03-bc3b-44b6-b2bb-f710ac05aa12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NVNWpfSaZNeQ",
        "colab_type": "code",
        "outputId": "c68051c8-b01f-49d2-822b-6843de3bcfa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Check whether Google Drive is connected\n",
        "\n",
        "with open('/gdrive/My Drive/test.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/My Drive/test.txt'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Google Drive!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Vswl4a7qZRPD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Connecting to Kaggle API via token and showing all available dataset\n",
        "\n",
        "#Note: Yout can view how to download Kaggle API token here - https://github.com/Kaggle/kaggle-api\n",
        "\n",
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "!cp \"/gdrive/My Drive/Deep Learning Workshop/kaggle.json\" ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ieFbRWIi6B_v",
        "colab_type": "code",
        "outputId": "6e23b57a-d2bb-464e-c165-ea33696d8d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "#Test that the kaggle command is working\n",
        "!kaggle datasets list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref                                                          title                                                size  lastUpdated          downloadCount  \n",
            "-----------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  \n",
            "ronitf/heart-disease-uci                                     Heart Disease UCI                                     3KB  2018-06-25 11:33:56          12228  \n",
            "russellyates88/suicide-rates-overview-1985-to-2016           Suicide Rates Overview 1985 to 2016                 396KB  2018-12-01 19:18:25           8638  \n",
            "karangadiya/fifa19                                           FIFA 19 complete player dataset                       2MB  2018-12-21 03:52:59          11505  \n",
            "iarunava/cell-images-for-detecting-malaria                   Malaria Cell Images Dataset                         337MB  2018-12-05 05:40:21           2156  \n",
            "mohansacharya/graduate-admissions                            Graduate Admissions                                   9KB  2018-12-28 10:07:14          11711  \n",
            "bigquery/crypto-ethereum-classic                             Ethereum Classic Blockchain                          69GB  2019-03-04 14:57:33              0  \n",
            "rmisra/news-headlines-dataset-for-sarcasm-detection          News Headlines Dataset For Sarcasm Detection          2MB  2018-06-09 22:14:56           1502  \n",
            "jessicali9530/stanford-dogs-dataset                          Stanford Dogs Dataset                               735MB  2019-02-13 05:45:25            851  \n",
            "lava18/google-play-store-apps                                Google Play Store Apps                                2MB  2019-02-03 13:55:47          39660  \n",
            "noriuk/us-education-datasets-unification-project             U.S. Education Datasets: Unification Project         85MB  2019-03-02 18:41:52           1689  \n",
            "vjchoudhary7/customer-segmentation-tutorial-in-python        Mall Customer Segmentation Data                       2KB  2018-08-11 07:23:02           3490  \n",
            "safegraph/census-block-group-american-community-survey-data  Census Block Group American Community Survey Data     2GB  2018-12-22 00:29:56            407  \n",
            "cityofLA/los-angeles-parking-citations                       Los Angeles Parking Citations                       245MB  2019-02-26 22:18:28           2050  \n",
            "mdhrumil/top-5000-youtube-channels-data-from-socialblade     Top 5000 Youtube channels data from Socialblade.    128KB  2018-09-09 14:05:54           4007  \n",
            "jessicali9530/celeba-dataset                                 CelebFaces Attributes (CelebA) Dataset                1GB  2018-06-01 20:08:48           4581  \n",
            "safegraph/visit-patterns-by-census-block-group               Consumer & Visitor Insights For Neighborhoods        66MB  2018-12-19 21:31:50            639  \n",
            "jutrera/stanford-car-dataset-by-classes-folder               Stanford Car Dataset by classes folder                2GB  2018-07-02 07:35:45           1920  \n",
            "fivethirtyeight/fivethirtyeight-comic-characters-dataset     FiveThirtyEight Comic Characters Dataset            577KB  2019-02-01 15:02:23           1363  \n",
            "anokas/kuzushiji                                             Kuzushiji-MNIST                                     318MB  2018-12-17 01:19:31            473  \n",
            "pavansanagapati/urban-sound-classification                   Urban Sound Classification                            6GB  2018-06-16 13:44:36           1574  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v8BpOqle6Oej",
        "colab_type": "code",
        "outputId": "a60746ff-706c-4d3e-f885-e3fcf3e6c7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "#Download the Kaggle NLP dataset that you are interested in\n",
        "#In my case, I am downloading the quora insincere question classification dataset\n",
        "\n",
        "!kaggle competitions download -c quora-insincere-questions-classification"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train.csv.zip to /content\n",
            " 75% 41.0M/54.4M [00:02<00:01, 9.05MB/s]\n",
            "100% 54.4M/54.4M [00:02<00:00, 19.9MB/s]\n",
            "Downloading embeddings.zip to /content\n",
            "100% 5.96G/5.96G [02:27<00:00, 59.9MB/s]\n",
            "\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "100% 4.08M/4.08M [00:00<00:00, 10.0MB/s]\n",
            "\n",
            "Downloading test.csv.zip to /content\n",
            " 83% 13.0M/15.7M [00:00<00:00, 13.2MB/s]\n",
            "100% 15.7M/15.7M [00:00<00:00, 24.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GW-aVmzb7shC",
        "colab_type": "code",
        "outputId": "70906c65-0148-4c1a-8ecf-ea493242e619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al\n",
        "\n",
        "#!unzip embeddings.zip (I'm not unzipping the embeddings since I won't be using it in my approach)\n",
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 6321976\n",
            "drwxr-xr-x 1 root root       4096 Mar  5 00:54 .\n",
            "drwxr-xr-x 1 root root       4096 Mar  5 00:51 ..\n",
            "drwxr-xr-x 1 root root       4096 Feb 26 17:33 .config\n",
            "-rw-r--r-- 1 root root 6395920052 Mar  5 00:54 embeddings.zip\n",
            "drwxr-xr-x 1 root root       4096 Feb 26 17:33 sample_data\n",
            "-rw-r--r-- 1 root root    4282631 Mar  5 00:54 sample_submission.csv.zip\n",
            "-rw-r--r-- 1 root root   16426497 Mar  5 00:54 test.csv.zip\n",
            "-rw-r--r-- 1 root root   57047694 Mar  5 00:51 train.csv.zip\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "szAzitP7i-Yl",
        "colab_type": "code",
        "outputId": "912f1b1c-64d3-40c1-ace2-8d712d098139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 6477464\n",
            "drwxr-xr-x 1 root root       4096 Mar  5 00:54 .\n",
            "drwxr-xr-x 1 root root       4096 Mar  5 00:51 ..\n",
            "drwxr-xr-x 1 root root       4096 Feb 26 17:33 .config\n",
            "-rw-r--r-- 1 root root 6395920052 Mar  5 00:54 embeddings.zip\n",
            "drwxr-xr-x 1 root root       4096 Feb 26 17:33 sample_data\n",
            "-rw-r--r-- 1 root root    4282631 Mar  5 00:54 sample_submission.csv.zip\n",
            "---------- 1 root root   35011536 Feb  6 00:46 test.csv\n",
            "-rw-r--r-- 1 root root   16426497 Mar  5 00:54 test.csv.zip\n",
            "---------- 1 root root  124206772 Oct 30 16:56 train.csv\n",
            "-rw-r--r-- 1 root root   57047694 Mar  5 00:51 train.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fRAG9nu5jTYh",
        "colab_type": "code",
        "outputId": "9cd53d1f-c7ab-4a0d-c5ea-6342a460b2c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Import relevant libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction, BertAdam\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm_notebook.pandas(desc='Progress')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5VK_ePvJZgIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing for CUDA"
      ]
    },
    {
      "metadata": {
        "id": "fUKsz97vZfMJ",
        "colab_type": "code",
        "outputId": "211bd620-6b25-4e97-9878-8d49af7e822f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ytGE2n7ejMqT",
        "colab_type": "code",
        "outputId": "60623d29-4cd2-4a0f-cb5d-39e0a9749c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#Since there are a lot of data in the dataset, I will be subsetting it for debugging/testing of the model first. Else, it will take a long time for my model to train\n",
        "#REMOVE: can remove this line of code when done testing\n",
        "\n",
        "raw_df = pd.read_csv('train.csv')\n",
        "print(len(raw_df[raw_df['target'] == 1]))\n",
        "print(len(raw_df[raw_df['target'] == 0]))\n",
        "\n",
        "pos_df = raw_df[raw_df['target'] == 1].iloc[:100, :]\n",
        "neg_df = raw_df[raw_df['target'] == 0].iloc[:100, :]\n",
        "\n",
        "short_df = pos_df.append(neg_df)\n",
        "\n",
        "short_df.to_csv('train_short.csv')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80810\n",
            "1225312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QsUFpNpiIu72",
        "colab_type": "code",
        "outputId": "04632bd1-5d28-485f-c53b-0ecd776a37ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "#Activate the logger for more information on what's happening\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmp2g6j2157\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 408034.90B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmp2g6j2157 to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmp2g6j2157\n",
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y1CttdtHrqfL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining the Dataset class to load the NLP dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, df_path, maxlen):\n",
        "    \n",
        "    'Initialization'\n",
        "    \n",
        "    #This will determine the max length of your tensor. If your tensor length < max length, it will be padded with 0.\n",
        "    #The rational is to have the same tensor length being passed into the model for more efficient computation.\n",
        "    \n",
        "    self.maxlen = maxlen\n",
        "    \n",
        "    #For simplicity, we will remove the indexes where the question is more than 515 in length (Bert has a limit of 515)\n",
        "    self.df = pd.read_csv(df_path).drop(59428, axis = 0).drop(205748, axis = 0).drop(163583, axis = 0).drop(443216, axis = 0) .reset_index()\n",
        "    \n",
        "    \n",
        "    self.df.labels = self.df.target\n",
        "    self.df.text = self.df.question_text\n",
        "    \n",
        "    #Tokenize the questions\n",
        "    \n",
        "    print('Start Tokenizing')\n",
        "    self.df.text = self.df.text.apply(tokenizer.tokenize) #.progress_apply(tokenizer.tokenize)\n",
        "    \n",
        "    #Index the tokens \n",
        "    \n",
        "    print('Start Indexing Tokens')\n",
        "    self.df.text = self.df.text.apply(tokenizer.convert_tokens_to_ids) #progress_apply(tokenizer.convert_tokens_to_ids)\n",
        "    \n",
        "    #Pad the text_index with 0 so that it hits the max_len\n",
        "    \n",
        "    print('Start Padding Process')\n",
        "    self.df.text = self.df.text.apply(self.pad_data) #progress_apply(self.pad_data) \n",
        "    \n",
        "    #Converting all numpy array (for text) to tensor\n",
        "    \n",
        "    print('Converting numpy array to tensor')\n",
        "    self.df.text = self.df.text.apply(torch.from_numpy) #progress_apply(torch.from_numpy)\n",
        "    \n",
        "    #Note: I am overwritting the column to reduce memory usage. If you prefer, you can create new columns for each step (tokenizing, indexing, padding)\n",
        "    \n",
        "    '''print('Start Tokenizing')\n",
        "    self.df.text_token = self.df.text.apply(tokenizer.tokenize) #.progress_apply(tokenizer.tokenize)\n",
        "    \n",
        "    #Index the tokens \n",
        "    \n",
        "    print('Start Indexing Tokens')\n",
        "    self.df.text_idx = self.df.text_token.apply(tokenizer.convert_tokens_to_ids) #progress_apply(tokenizer.convert_tokens_to_ids)\n",
        "    \n",
        "    #Pad the text_index with 0 so that it hits the max_len\n",
        "    \n",
        "    print('Start Padding Process')\n",
        "    self.df.text_idx_padded = self.df.text_idx.apply(self.pad_data) #progress_apply(self.pad_data) \n",
        "    \n",
        "    #Converting all numpy array (for text) to tensor\n",
        "    \n",
        "    print('Converting numpy array to tensor')\n",
        "    self.df.text_idx_padded = self.df.text_idx_padded.apply(torch.from_numpy) #progress_apply(torch.from_numpy)\n",
        "    \n",
        "    #drop the text_token and token_indexing to reduce memory usage\n",
        "    self.df = self.df.drop('text', axis = 1)'''\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the total number of samples'\n",
        "    return len(self.df.text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generates one sample of data'\n",
        "    # Select sample\n",
        "    text_idx = self.df.text[index]\n",
        "    labels = self.df.labels[index]\n",
        "\n",
        "    return text_idx, labels\n",
        "   \n",
        "  def pad_data(self, s):\n",
        "    #Pad the tensor with zeros so that all tensors have the same length.\n",
        "    padded = np.zeros((self.maxlen,), dtype=np.int64)\n",
        "    if len(s) > self.maxlen: \n",
        "      padded[:] = s[:self.maxlen]\n",
        "    else: padded[:len(s)] = s\n",
        "    return padded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccSKhhldwqCH",
        "colab_type": "code",
        "outputId": "87c81452-5b18-4a5b-b99b-e74d73543fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "quora_df = Dataset('train.csv', maxlen = 178)\n",
        "\n",
        "#For debugging: \n",
        "#quora_df = Dataset('train_short.csv', maxlen = 178)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start Tokenizing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vRE2dYbuBw6M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWclhGkBXfl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining the Bert model \n",
        "\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased').to(device) #BertModel\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "  \n",
        "'''model.fc = nn.Sequential(\n",
        "    nn.Linear(178, 2048),\n",
        "    nn.Sigmoid(),\n",
        "    #nn.Dropout(0.1),\n",
        "    nn.Linear(2048, 1024),\n",
        "    nn.Sigmoid(),\n",
        "    #nn.Dropout(0.1),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.Sigmoid(),\n",
        "    #nn.Dropout(0.1),\n",
        "    nn.Linear(512, 2)).to(device)'''\n",
        "\n",
        "'''model.fc = nn.Sequential(\n",
        "    nn.Embedding(178, 178),\n",
        "    nn.LayerNorm(178, 512),\n",
        "    nn.Hardshrink(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.Hardshrink(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.Hardshrink(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(128, 2)).to(device)'''\n",
        "\n",
        "'''model.fc = nn.Sequential(\n",
        "    nn.BatchNorm1d(178, 178),\n",
        "    nn.Linear(178, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(256, 2)).to(device)'''\n",
        "\n",
        "'''model.fc = nn.Sequential(\n",
        "    nn.BatchNorm1d(178, 178),\n",
        "    nn.Linear(178, 512),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(256, 2)).to(device)'''\n",
        "\n",
        "'''model.fc = nn.Sequential(\n",
        "    nn.Linear(178, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(256, 2)).to(device)'''\n",
        "\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(178, 178),\n",
        "    nn.Hardshrink(),\n",
        "    #nn.Dropout(0.2),\n",
        "    nn.Linear(178, 2)).to(device)\n",
        "\n",
        "class_weight = torch.FloatTensor([1, 17]).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weight)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = BertAdam(model.fc.parameters(), lr = 0.02)\n",
        "\n",
        "#optimizer = BertAdam(model.parameters(), lr = 0.01)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8OwjMxBI0TpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_num(dataset, train_split = 0.7):  \n",
        "  dataset_len = len(dataset)  # To check how many elements there are in the dataset\n",
        "  \n",
        "  #train_ and test_ split based on number of elements in the dataset\n",
        "  train_ = round(dataset_len * train_split)\n",
        "  test_ = round(dataset_len * (1 - train_split))\n",
        "  \n",
        "  return (train_, test_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCuxySRGHJn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs, file_name):\n",
        "  \n",
        "  max_epochs = num_epochs \n",
        "\n",
        "  min_validation_loss = np.Inf\n",
        "  min_validation_acc = 0\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "\n",
        "      print('Epoch', epoch)\n",
        "      print('-' * 20)\n",
        "      print('')\n",
        "\n",
        "      # Training\n",
        "\n",
        "      model.train() \n",
        "\n",
        "      records = 0\n",
        "      train_running_loss = 0.0\n",
        "      train_running_corrects = 0\n",
        "\n",
        "      for inputs, labels in train_dataloaders:\n",
        "          \n",
        "          if records % 100000 == 0:\n",
        "            print('Training in progress:', '-------->' , records, 'out of', len(train_dataloaders.dataset))\n",
        "          \n",
        "          # Transfer to GPU\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          # forward + backward + optimize\n",
        "          bert_output = model.fc(inputs.float())\n",
        "          #bert_output = model(inputs)\n",
        "          #prob = torch.sigmoid(bert_output)\n",
        "          \n",
        "          loss = criterion(bert_output, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_running_loss += loss.item()\n",
        "          #_, preds = torch.max(prob, 1)\n",
        "          _, preds = torch.max(bert_output, 1)\n",
        "          \n",
        "          train_running_corrects += torch.sum(preds == labels.long())\n",
        "\n",
        "          records += train_dataloaders.batch_size\n",
        "\n",
        "      epoch_loss = train_running_loss / records\n",
        "      epoch_acc = train_running_corrects.item() / records\n",
        "\n",
        "      print('Training loss: {:.4f}, Training accuracy: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "      print('')\n",
        "\n",
        "      test_correct = 0\n",
        "      test_total = 0\n",
        "      test_running_loss = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "          model.eval()\n",
        "\n",
        "          for inputs, labels in test_dataloaders:\n",
        "              \n",
        "              if test_total % 100000 == 0:\n",
        "                print('Validation in progress:', '------>', test_total, 'out of', len(test_dataloaders.dataset))\n",
        "              \n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              \n",
        "              outputs = model.fc(inputs.float())\n",
        "              #outputs = model.fc(inputs)\n",
        "\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "              test_running_loss += loss.item()\n",
        "\n",
        "              test_total += labels.size(0)\n",
        "              test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "          val_loss = test_running_loss / test_total\n",
        "          val_acc = test_correct / test_total\n",
        "              \n",
        "      print('Validation loss: {:4f}'.format(val_loss))\n",
        "      print('Validation accuracy: {:4f}'.format(val_acc))\n",
        "      print('')\n",
        "\n",
        "      #if (val_loss < min_validation_loss) & (val_acc > min_validation_acc):\n",
        "      if (val_acc > min_validation_acc):\n",
        "\n",
        "        #Update min_validation_loss and min_validation_acc if both validation accuracy and validation loss improves \n",
        "        #min_validation_loss = val_loss\n",
        "        min_validation_acc = val_acc\n",
        "\n",
        "        #Save the model weights if both validation accuracy and validation loss improves \n",
        "        torch.save(model.state_dict(), file_name)\n",
        "        print('Model validation loss < previous model. Model saved')\n",
        "        print('')\n",
        "        \n",
        "      print('-' * 20)\n",
        "      print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ErCYCxi0giX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set, test_set = random_split(quora_df, split_num(quora_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dXPNwHewyxmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Using the image datasets and the trainforms, define the dataloaders\n",
        "train_dataloaders = DataLoader(\n",
        "            train_set,\n",
        "            batch_size=30000,\n",
        "            shuffle=True,\n",
        "            num_workers=4)\n",
        "\n",
        "test_dataloaders = DataLoader(\n",
        "            test_set,\n",
        "            batch_size=30000,\n",
        "            shuffle=True,\n",
        "            num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yu5-ZgDd7PvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = '/gdrive/My Drive/Deep Learning Workshop/Advanced NLP Sequencing/Project/Bert.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q1jPDX5KqCNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load model checkpoint so that we don't have to re-run the training\n",
        "#model.load_state_dict(torch.load(file_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8QE3zKw698g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model(model, criterion, optimizer, 2000, file_name)\n",
        "\n",
        "#DataLoader affects how much CUDA memory is being used"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8uhNmhCiOmJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}